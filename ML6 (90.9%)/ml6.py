# -*- coding: utf-8 -*-
"""ML9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XNTqqeQgYsrws0bhHS6t9LqEDO3VTS0Q
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install -q kaggle

from google.colab import files
files.upload()  

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle competitions download -c deepfake-classification-unibuc
!unzip -q deepfake-classification-unibuc.zip -d deepfake_data

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

from tensorflow.keras import layers, models
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping

BASE_DIR = "deepfake_data"
TRAIN_IMG_DIR = os.path.join(BASE_DIR, "train")
TEST_IMG_DIR = os.path.join(BASE_DIR, "test")
VALIDATION_IMG_DIR = os.path.join(BASE_DIR, "validation")
TRAIN_CSV_PATH = os.path.join(BASE_DIR, "train.csv")
VALIDATION_CSV_PATH = os.path.join(BASE_DIR, "validation.csv")

IMG_SIZE = (128, 128)
BATCH_SIZE = 32
EPOCHS = 80

train_df = pd.read_csv(TRAIN_CSV_PATH)
train_df["image_id"] = train_df["image_id"].astype(str) + ".png"
train_df["label"] = train_df["label"].astype(str) 

val_df = pd.read_csv(VALIDATION_CSV_PATH)
val_df["image_id"] = val_df["image_id"].astype(str) + ".png"
val_df["label"] = val_df["label"].astype(str) 

import numpy as np
import pandas as pd
from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_df = pd.read_csv(TRAIN_CSV_PATH)
train_df["image_id"] = train_df["image_id"].astype(str) + ".png"
train_df["label"] = train_df["label"].astype(str)

df_class_4 = train_df[train_df["label"] == "4"]
df_other = train_df[train_df["label"] != "4"]

#Am revenit la proportii egale de imagini

df_augmented = pd.concat([
    df_class_4] * 2 +
    [df_other] * 2,
    ignore_index=True
)

train_gen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    zoom_range=0.3,
    horizontal_flip=True,
    fill_mode='nearest'
)

val_gen = ImageDataGenerator(rescale=1./255)
test_gen = ImageDataGenerator(rescale=1./255)

train_data = train_gen.flow_from_dataframe(
    df_augmented,
    directory=TRAIN_IMG_DIR,
    x_col="image_id",
    y_col="label",
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="categorical",
    shuffle=True
)



val_data = val_gen.flow_from_dataframe(
    val_df,
    directory=VALIDATION_IMG_DIR,
    x_col="image_id",
    y_col="label",
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="categorical",
    shuffle=False
)

from tensorflow.keras import layers, models, regularizers, Input
def create_optimized_cnn(input_shape, num_classes, l2_lambda=1e-5):
    inputs = Input(shape=input_shape)

    x = layers.Conv2D(32, 3, padding='same',
                      kernel_regularizer=regularizers.l2(l2_lambda))(inputs)
    x = layers.ReLU()(x)
    x = layers.Dropout(0.1)(x)

    x = layers.Conv2D(70, 5, padding='same',
                      kernel_regularizer=regularizers.l2(l2_lambda))(x)
    x = layers.ReLU()(x)
    x = layers.MaxPooling2D()(x)
    x = layers.Dropout(0.1)(x)

    x = layers.Conv2D(160, 3, padding='same',
                      kernel_regularizer=regularizers.l2(l2_lambda))(x)
    x = layers.ReLU()(x)
    x = layers.MaxPooling2D()(x)
    x = layers.Dropout(0.15)(x)

    x = layers.Flatten()(x)

    skip = layers.Conv2D(48, 5, padding='same',
                         kernel_regularizer=regularizers.l2(l2_lambda))(inputs)
    skip = layers.ReLU()(skip)
    skip = layers.AveragePooling2D(pool_size=(16, 16), strides=(16, 16))(skip)
    skip = layers.Flatten()(skip)

    x = layers.Concatenate()([x, skip])

    x = layers.Dense(420,
                     kernel_regularizer=regularizers.l2(l2_lambda))(x)
    x = layers.ReLU()(x)
    x = layers.Dropout(0.3)(x)

    x = layers.Dense(256,
                     kernel_regularizer=regularizers.l2(l2_lambda))(x)
    x = layers.ReLU()(x)
    x = layers.Dropout(0.15)(x)

    x = layers.Dense(64,
                     kernel_regularizer=regularizers.l2(l2_lambda))(x)
    x = layers.ReLU()(x)
    x = layers.Dropout(0.15)(x)

    outputs = layers.Dense(num_classes, activation='softmax')(x)

    return models.Model(inputs, outputs)



import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix
import seaborn as sns
from tensorflow.keras.optimizers.schedules import ExponentialDecay
from tensorflow.keras.optimizers import Adam

IMG_SIZE = (128, 128)
NUM_CLASSES = 5

model = create_optimized_cnn((*IMG_SIZE, 3), NUM_CLASSES)


lr_schedule = ExponentialDecay(
    initial_learning_rate=5*(1e-4),
    decay_steps=1000,
    decay_rate=0.95,        # Am facut rata de invatare sa scada.  
    staircase=False
)

model.compile(
    optimizer=Adam(learning_rate=lr_schedule),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

early_stop = EarlyStopping(
    monitor='val_accuracy',
    patience=13,
    restore_best_weights=True
)

history = model.fit(
    train_data,
    validation_data=val_data,
    epochs=EPOCHS,
    callbacks=[early_stop]
)

plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train')
plt.plot(history.history['val_accuracy'], label='Validation')
plt.title("Accuracy per Epoch")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.grid()
plt.savefig("accuracy_plot.png")

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train')
plt.plot(history.history['val_loss'], label='Validation')
plt.title("Loss per Epoch")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.grid()
plt.savefig("loss_plot.png")
plt.close()

val_preds = model.predict(val_data)
val_preds_labels = np.argmax(val_preds, axis=1)
true_labels = val_data.classes

cm = confusion_matrix(true_labels, val_preds_labels)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.savefig("confusion_matrix.png")
plt.close()


test_filenames = sorted([f for f in os.listdir(TEST_IMG_DIR) if f.endswith(".png")])
test_df = pd.DataFrame({"filename": test_filenames})

test_gen = ImageDataGenerator(rescale=1./255)

test_data = test_gen.flow_from_dataframe(
    test_df,
    directory=TEST_IMG_DIR,
    x_col="filename",
    class_mode=None,
    target_size=IMG_SIZE,
    batch_size=1,
    shuffle=False
)

preds = model.predict(test_data, verbose=1)
predicted_labels = preds.argmax(axis=1)

submission = pd.DataFrame({
    "image_id": test_df["filename"].str.replace(".png", "", regex=False),
    "label": predicted_labels
})

submission.to_csv("submission.csv", index=False)
submission.head()

test_filenames = sorted([f for f in os.listdir(TEST_IMG_DIR) if f.endswith(".png")])
test_df = pd.DataFrame({"filename": test_filenames})

test_gen = ImageDataGenerator(rescale=1./255)

test_data = test_gen.flow_from_dataframe(
    test_df,
    directory=TEST_IMG_DIR,
    x_col="filename",
    class_mode=None,
    target_size=IMG_SIZE,
    batch_size=1,
    shuffle=False
)

preds = model.predict(test_data, verbose=1)
predicted_labels = preds.argmax(axis=1)

submission = pd.DataFrame({
    "image_id": test_df["filename"].str.replace(".png", "", regex=False),
    "label": predicted_labels
})

submission.to_csv("submission.csv", index=False)
submission.head()