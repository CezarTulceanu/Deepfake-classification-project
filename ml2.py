# -*- coding: utf-8 -*-
"""ML5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y61MUEs0kIFJxcm472b_F6kEym-ljNc_
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install -q kaggle

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle competitions download -c deepfake-classification-unibuc
!unzip -q deepfake-classification-unibuc.zip -d deepfake_data

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

from tensorflow.keras import layers, models
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping

BASE_DIR = "deepfake_data"
TRAIN_IMG_DIR = os.path.join(BASE_DIR, "train")
TEST_IMG_DIR = os.path.join(BASE_DIR, "test")
VALIDATION_IMG_DIR = os.path.join(BASE_DIR, "validation")
TRAIN_CSV_PATH = os.path.join(BASE_DIR, "train.csv")
VALIDATION_CSV_PATH = os.path.join(BASE_DIR, "validation.csv")

IMG_SIZE = (128, 128)
BATCH_SIZE = 32
EPOCHS = 10



train_df = pd.read_csv(TRAIN_CSV_PATH)
train_df["image_id"] = train_df["image_id"].astype(str) + ".png"
train_df["label"] = train_df["label"].astype(str)

val_df = pd.read_csv(VALIDATION_CSV_PATH)
val_df["image_id"] = val_df["image_id"].astype(str) + ".png" 
val_df["label"] = val_df["label"].astype(str)

#Tot nu facem nicio augmentare

train_gen = ImageDataGenerator(rescale=1./255)
val_gen = ImageDataGenerator(rescale=1./255)

train_data = train_gen.flow_from_dataframe(
    train_df,
    directory=TRAIN_IMG_DIR,
    x_col="image_id",
    y_col="label",
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="categorical"
)

val_data = val_gen.flow_from_dataframe(
    val_df,
    directory=VALIDATION_IMG_DIR,
    x_col="image_id",
    y_col="label",
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="categorical",
    shuffle=False
)

from tensorflow.keras import layers, models, regularizers, Input
from tensorflow.keras.callbacks import EarlyStopping

#Aici am adaugat dropout pentru a reduce overfittingul masiv din prima incercare

def create_simple_cnn(input_shape, num_classes):
    inputs = Input(shape=input_shape)

    x = layers.Conv2D(32, 3, activation='relu', padding='same')(inputs)
    x = layers.MaxPooling2D()(x)
    x = layers.Dropout(0.1)(x)

    x = layers.Conv2D(64, 5, activation='relu', padding='same')(x)
    x = layers.MaxPooling2D()(x)
    x = layers.Dropout(0.2)(x)

    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)
    x = layers.MaxPooling2D()(x)
    x = layers.Dropout(0.3)(x)

    x = layers.Flatten()(x)
    x = layers.Dense(128, activation='relu')(x)
    x = layers.Dropout(0.4)(x)

    x = layers.Dense(64, activation='relu')(x)
    x = layers.Dropout(0.1)(x)

    outputs = layers.Dense(num_classes, activation='softmax')(x)

    return models.Model(inputs, outputs)

import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix
import seaborn as sns

IMG_SIZE = (128, 128)
NUM_CLASSES = 5

model = create_simple_cnn((*IMG_SIZE, 3), NUM_CLASSES)

model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

#Am adaugat si early stopping pentru a obtine cea mai buna versiune a modelului

#Am setat rabdarea la 3 pentru inceput

early_stop = EarlyStopping(
    monitor='val_loss',
    patience=3,
    restore_best_weights=True
)

#Antrenam modelul
history = model.fit(
    train_data,
    validation_data=val_data,
    epochs=EPOCHS,
    callbacks=[early_stop]
)

#Salvam graficele
plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train')
plt.plot(history.history['val_accuracy'], label='Validation')
plt.title("Accuracy per Epoch")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.grid()
plt.savefig("accuracy_plot.png")

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train')
plt.plot(history.history['val_loss'], label='Validation')
plt.title("Loss per Epoch")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.grid()
plt.savefig("loss_plot.png")
plt.close()

# Confusion matrix
val_preds = model.predict(val_data)
val_preds_labels = np.argmax(val_preds, axis=1)
true_labels = val_data.classes

cm = confusion_matrix(true_labels, val_preds_labels)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.savefig("confusion_matrix.png")
plt.close()

test_filenames = sorted([f for f in os.listdir(TEST_IMG_DIR) if f.endswith(".png")])
test_df = pd.DataFrame({"filename": test_filenames})

test_gen = ImageDataGenerator(rescale=1./255)

test_data = test_gen.flow_from_dataframe(
    test_df,
    directory=TEST_IMG_DIR,
    x_col="filename",
    class_mode=None,
    target_size=IMG_SIZE,
    batch_size=1,
    shuffle=False
)

preds = model.predict(test_data, verbose=1)
predicted_labels = preds.argmax(axis=1)

submission = pd.DataFrame({
    "image_id": test_df["filename"].str.replace(".png", "", regex=False),
    "label": predicted_labels
})

submission.to_csv("submission.csv", index=False)
submission.head()